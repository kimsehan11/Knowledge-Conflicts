{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7117b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb91b82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be201aba1ca9417aa108ad4b6a28743f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"/home/sehan/sehan_workspace/llama-3.1-8b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    dtype=\"auto\",\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "egy30slkpw",
   "source": "# 간단한 텍스트 생성 테스트\nprompt = \"What is artificial intelligence?\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(\n    **inputs,\n    max_new_tokens=100,\n    temperature=0.7,\n    top_p=0.9,\n    do_sample=True,\n)\n\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qmhseauylhf",
   "source": "# RAG를 위한 벡터 데이터베이스 설정 (ChromaDB 사용)\nfrom langchain_community.document_loaders import TextLoader, DirectoryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\n\n# 문서 로딩 (예시 - 실제 문서 경로로 변경 필요)\n# loader = DirectoryLoader('./documents/', glob=\"**/*.txt\")\n# documents = loader.load()\n\n# 예시 문서 생성\nfrom langchain.schema import Document\n\ndocuments = [\n    Document(page_content=\"Paris is the capital of France. It is known for the Eiffel Tower.\", metadata={\"source\": \"geography\"}),\n    Document(page_content=\"Python is a popular programming language. It is widely used for AI and data science.\", metadata={\"source\": \"tech\"}),\n    Document(page_content=\"The Louvre Museum is located in Paris and houses the Mona Lisa.\", metadata={\"source\": \"culture\"}),\n]\n\n# 텍스트 분할\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=50\n)\nsplits = text_splitter.split_documents(documents)\n\n# 임베딩 모델 설정\nembeddings = HuggingFaceEmbeddings(\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n)\n\n# 벡터 데이터베이스 생성\nvectorstore = Chroma.from_documents(\n    documents=splits,\n    embedding=embeddings,\n    persist_directory=\"./RAG/vectorDB\"\n)\n\nprint(f\"벡터 데이터베이스 생성 완료! 총 {len(splits)}개의 문서 청크 저장됨\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledge_conflict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}